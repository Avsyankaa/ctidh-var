/* DO NOT EDIT! generated by ./autogen */

.intel_syntax noprefix

#include "uintbig_namespace.h"
#include "fp_namespace.h"

.section .rodata

.set pbits,1020
.set pbytes,128
.set plimbs,16
.inv_min_p_mod_r: /* -p^-1 mod 2^64 */
    .quad 0xd2c2c24160038025

.global fp_0
fp_0:
    .zero 128

.global fp_1
fp_1: /* 2^1024 mod p */
    .quad 0x65e7ee6590e6567d, 0x40a5f2587fef86d4, 0x99f9e607b99d62f2, 0x1089df50f4f8f26d
    .quad 0x592890dd02bb585a, 0xe1b6be68b969ecb9, 0xaebe3c10395f33c3, 0x5ef9652396531f1b
    .quad 0x28d37db76b7a1b7f, 0x86d089fa474b4a3f, 0xdbce120cc7a4fff2, 0x08b3f947137340ac
    .quad 0x913f3e7c71b37ce5, 0xc7d1b17b09ec4577, 0x9d834aff6f7956b6, 0x044c4b3e968ec2b8

.global fp_2
fp_2: /* 2^1025 mod p */
    .quad 0xcbcfdccb21ccacfa, 0x814be4b0ffdf0da8, 0x33f3cc0f733ac5e4, 0x2113bea1e9f1e4db
    .quad 0xb25121ba0576b0b4, 0xc36d7cd172d3d972, 0x5d7c782072be6787, 0xbdf2ca472ca63e37
    .quad 0x51a6fb6ed6f436fe, 0x0da113f48e96947e, 0xb79c24198f49ffe5, 0x1167f28e26e68159
    .quad 0x227e7cf8e366f9ca, 0x8fa362f613d88aef, 0x3b0695fedef2ad6d, 0x0898967d2d1d8571

.r_squared_mod_p: /* (2^1024)^2 mod p */
    .quad 0xd6b8f146ec5055af, 0x68ac5d7707ccb03a, 0x1322c9b9837dca17, 0x4f2940830c1d2b35
    .quad 0x8c1a56e5bf96471a, 0x6cdde00636c4f801, 0x9365ec4fa327c9ac, 0xa0056a67c1de0e82
    .quad 0x8aa6fa7e6811faa8, 0x9aad9631bb760403, 0x156b34c683839b9d, 0xa5ae047480992b2c
    .quad 0xc124d930289048b5, 0x4f8a8344bbe56288, 0xe1a2eb1d838b8237, 0x057162f911ca93a3

.section .data
.global fp_mulsq_count
fp_mulsq_count:
    .quad 0
.global fp_sq_count
fp_sq_count:
    .quad 0
.global fp_addsub_count
fp_addsub_count:
    .quad 0

.section .text
.p2align 4,,15

.global fp_copy
fp_copy:
    cld
    mov rcx, plimbs
    rep movsq
    ret

.global fp_cmov
fp_cmov:
    movzx rax, dl
    neg rax
    .set k, 0
    .rept plimbs
        mov rcx, [rdi + 8*k]
        mov rdx, [rsi + 8*k]

        xor rdx, rcx
        and rdx, rax
        xor rcx, rdx

        mov [rdi + 8*k], rcx

        .set k, k+1
    .endr
    ret

.global fp_cswap
fp_cswap:
    movzx rax, dl
    neg rax
    .set k, 0
    .rept plimbs
        mov rcx, [rdi + 8*k]
        mov rdx, [rsi + 8*k]

        mov r8, rcx
        xor r8, rdx
        and r8, rax

        xor rcx, r8
        xor rdx, r8

        mov [rdi + 8*k], rcx
        mov [rsi + 8*k], rdx

        .set k, k+1
    .endr
    ret

.reduce_once:
    push rbp
    sub rsp, 64
    mov rbp, rdi

    mov rdi, [rbp + 0]
    sub rdi, [rip + uintbig_p + 0]
    movq [rsp + 0], rdi
    mov rsi, [rbp + 8]
    sbb rsi, [rip + uintbig_p + 8]
    movq [rsp + 8], rsi
    mov rdx, [rbp + 16]
    sbb rdx, [rip + uintbig_p + 16]
    movq [rsp + 16], rdx
    mov rcx, [rbp + 24]
    sbb rcx, [rip + uintbig_p + 24]
    movq [rsp + 24], rcx
    mov r8,  [rbp + 32]
    sbb r8,  [rip + uintbig_p + 32]
    movq [rsp + 32], r8
    mov r9,  [rbp + 40]
    sbb r9,  [rip + uintbig_p + 40]
    movq [rsp + 40], r9
    mov r10, [rbp + 48]
    sbb r10, [rip + uintbig_p + 48]
    movq [rsp + 48], r10
    mov r11, [rbp + 56]
    sbb r11, [rip + uintbig_p + 56]
    movq [rsp + 56], r11
    mov rdi, [rbp + 64]
    sbb rdi, [rip + uintbig_p + 64]
    mov rsi, [rbp + 72]
    sbb rsi, [rip + uintbig_p + 72]
    mov rdx, [rbp + 80]
    sbb rdx, [rip + uintbig_p + 80]
    mov rcx, [rbp + 88]
    sbb rcx, [rip + uintbig_p + 88]
    mov r8,  [rbp + 96]
    sbb r8,  [rip + uintbig_p + 96]
    mov r9,  [rbp + 104]
    sbb r9,  [rip + uintbig_p + 104]
    mov r10, [rbp + 112]
    sbb r10, [rip + uintbig_p + 112]
    mov r11, [rbp + 120]
    sbb r11, [rip + uintbig_p + 120]

    setnc al
    movzx rax, al
    neg rax

.macro cswap2, r, m
    xor \r, \m
    and \r, rax
    xor \m, \r
.endm

    cswap2 rdi, [rbp + 64]
    cswap2 rsi, [rbp + 72]
    cswap2 rdx, [rbp + 80]
    cswap2 rcx, [rbp + 88]
    cswap2 r8, [rbp + 96]
    cswap2 r9, [rbp + 104]
    cswap2 r10, [rbp + 112]
    cswap2 r11, [rbp + 120]
    movq rdi, [rsp + 0]
    cswap2 rdi, [rbp + 0]
    movq rsi, [rsp + 8]
    cswap2 rsi, [rbp + 8]
    movq rdx, [rsp + 16]
    cswap2 rdx, [rbp + 16]
    movq rcx, [rsp + 24]
    cswap2 rcx, [rbp + 24]
    movq r8, [rsp + 32]
    cswap2 r8, [rbp + 32]
    movq r9, [rsp + 40]
    cswap2 r9, [rbp + 40]
    movq r10, [rsp + 48]
    cswap2 r10, [rbp + 48]
    movq r11, [rsp + 56]
    cswap2 r11, [rbp + 56]

    add rsp, 64
    pop rbp
    ret

.global fp_add2
fp_add2:
    mov rdx, rdi
.global fp_add3
fp_add3:
    addq [fp_addsub_count+rip],1
    push rdi
    call uintbig_add3
    pop rdi
    jmp .reduce_once

.global fp_sub2
fp_sub2:
  mov rdx, rdi
  xchg rsi, rdx
.global fp_sub3
fp_sub3:
    addq [fp_addsub_count+rip],1
    push rdi
    call uintbig_sub3
    pop rdi
    neg rax

    sub rsp, pbytes

    mov rcx, [rip + uintbig_p +  0]
    and rcx, rax
    mov [rsp + 0],rcx
    .set k, 1
    .rept plimbs-1
        mov rcx, [rip + uintbig_p + 8*k]
        and rcx, rax
        mov [rsp + 8*k], rcx
        .set k, k+1
    .endr

    mov rcx, [rsp +  0]
    add rcx, [rdi +  0]
    mov [rdi +  0], rcx
    .set k, 1
    .rept plimbs-1
        mov rcx, [rsp + 8*k]
        adc rcx, [rdi + 8*k]
        mov [rdi + 8*k], rcx
        .set k, k+1
    .endr

    add rsp, pbytes
    ret

#ifdef _ADX_

.macro MUL384_SCHOOL M0, M1, C, S, T0, T1, T2, T3, T4, T5, T6, T7 
    mov    rdx, \M0
    mulx   \T0, \T1, \M1    
    mulx   \T2, \T3, 8\M1
    mov    \C, \T1             // C0_final 
    xor    rax, rax
    mulx   \T4, \T5, 16\M1 
    adox   \T0, \T3               
    adox   \T2, \T5     
    mulx   \T1, \T3, 24\M1
    adox   \T4, \T3         
    mulx   \T5, \T6, 32\M1 
    adox   \T1, \T6        
    mulx   \T3, \T7, 40\M1    
    adox   \T5, \T7       
    adox   \T3, rax        
    
    mov    rdx, 8\M0 
    mulx   \T6, \T7, \M1 
    xor    rax, rax
    adcx   \T0, \T7 
    mov    8\C, \T0            // C1_final 
    adcx   \T2, \T6     
    mulx   \T6, \T7, 8\M1
    adox   \T2, \T7 
    adcx   \T4, \T6        
    mulx   \T0, \T6, 16\M1 
    adox   \T4, \T6  
    adcx   \T0, \T1     
    mulx   \T1, \T7, 24\M1   
    adcx   \T1, \T5  
    mulx   \T5, \T6, 32\M1     
    adcx   \T3, \T5   
    mulx   \T5, rdx, 40\M1
    adcx   \T5, rax 
        
    adox   \T0, \T7  
    adox   \T1, \T6  
    adox   \T3, rdx  
    adox   \T5, rax         
    
    mov    rdx, 16\M0 
    mulx   \T6, \T7, \M1
    xor    rax, rax 
    adcx   \T2, \T7 
    mov    16\C, \T2           // C2_final 
    adcx   \T4, \T6     
    mulx   \T6, \T7, 8\M1
    adox   \T4, \T7 
    adcx   \T0, \T6        
    mulx   \T2, \T6, 16\M1
    adox   \T0, \T6 
    adcx   \T1, \T2     
    mulx   \T2, \T7, 24\M1   
    adcx   \T3, \T2  
    mulx   \T2, \T6, 32\M1     
    adcx   \T5, \T2   
    mulx   \T2, rdx, 40\M1     
    adcx   \T2, rax 
         
    adox   \T1, \T7  
    adox   \T3, \T6  
    adox   \T5, rdx 
    adox   \T2, rax           
    
    mov    rdx, 24\M0 
    mulx   \T6, \T7, \M1
    xor    rax, rax 
    adcx   \T4, \T7 
    mov    24\C, \T4           // C3_final 
    adcx   \T0, \T6     
    mulx   \T6, \T7, 8\M1
    adox   \T0, \T7
    adcx   \T1, \T6        
    mulx   \T4, \T6, 16\M1
    adox   \T1, \T6  
    adcx   \T3, \T4     
    mulx   \T4, \T7, 24\M1   
    adcx   \T5, \T4  
    mulx   \T4, \T6, 32\M1     
    adcx   \T2, \T4   
    mulx   \T4, rdx, 40\M1     
    adcx   \T4, rax
        
    adox   \T3, \T7  
    adox   \T5, \T6  
    adox   \T2, rdx  
    adox   \T4, rax         
    
    mov    rdx, 32\M0 
    mulx   \T6, \T7, \M1 
    xor    rax, rax
    adcx   \T0, \T7 
    mov    32\C, \T0           // C4_final 
    adcx   \T1, \T6     
    mulx   \T6, \T7, 8\M1
    adox   \T1, \T7 
    adcx   \T3, \T6        
    mulx   \T0, \T6, 16\M1 
    adox   \T3, \T6 
    adcx   \T5, \T0     
    mulx   \T0, \T7, 24\M1   
    adcx   \T2, \T0  
    mulx   \T0, \T6, 32\M1     
    adcx   \T4, \T0   
    mulx   \T0, rdx, 40\M1     
    adcx   \T0, rax 
         
    adox   \T5, \T7  
    adox   \T2, \T6  
    adox   \T4, rdx  
    adox   \T0, rax           
    
    mov    rdx, 40\M0 
    mulx   \T6, \T7, \M1 
    xor    rax, rax
    adcx   \T1, \T7 
    mov    40\C, \T1           // C5_final 
    adcx   \T3, \T6     
    mulx   \T6, \T7, 8\M1
    adox   \T3, \T7 
    adcx   \T5, \T6        
    mulx   \T1, \T6, 16\M1
    adox   \T5, \T6 
    adcx   \T2, \T1     
    mulx   \T1, \T7, 24\M1   
    adcx   \T4, \T1  
    mulx   \T1, \T6, 32\M1     
    adcx   \T0, \T1   
    mulx   \T1, rdx, 40\M1     
    adcx   \T1, rax 
         
    adox   \T2, \T7 
    adox   \T4, \T6 
    adox   \T0, rdx 
    adox   \T1, rax 
    mov    48\C, \T3 
    mov    56\C, \T5 
    mov    64\C, \T2 
    mov    72\C, \T4
    mov    80\C, \T0 
    mov    88\C, \T1 
.endm

#else

.macro MUL384_SCHOOL M0, M1, C, S, T0, T1, T2, T3, T4, T5, T6, T7 
    mov    rdx, \M0
    mulx   \T0, \T1, \M1    
    mulx   \T2, \T3, 8\M1
    mov    \C, \T1             // C0_final 
    xor    rax, rax
    mulx   \T4, \T5, 16\M1 
    add    \T0, \T3               
    adc    \T2, \T5     
    mulx   \T1, \T3, 24\M1
    adc    \T4, \T3         
    mulx   \T5, \T6, 32\M1 
    adc    \T1, \T6        
    mulx   \T3, \T7, 40\M1    
    adc    \T5, \T7       
    adc    \T3, rax        
    
    mov    rdx, 8\M0 
    mulx   \T6, \T7, \M1 
    add    \T0, \T7 
    mov    8\C, \T0            // C1_final 
    adc    \T2, \T6     
    mulx   \T6, \T7, 8\M1
    mov    \S, \T7             // store T7
    adc    \T4, \T6        
    mulx   \T0, \T6, 16\M1   
    mov    8\S, \T6            // store T6 
    adc    \T0, \T1     
    mulx   \T1, \T7, 24\M1   
    adc    \T1, \T5  
    mulx   \T5, \T6, 32\M1     
    adc    \T3, \T5   
    mulx   \T5, rdx, 40\M1
    adc    \T5, rax 
        
    xor    rax, rax
    add    \T2, \S 
    adc    \T4, 8\S  
    adc    \T0, \T7  
    adc    \T1, \T6  
    adc    \T3, rdx  
    adc    \T5, rax         
    
    mov    rdx, 16\M0 
    mulx   \T6, \T7, \M1 
    add    \T2, \T7 
    mov    16\C, \T2           // C2_final 
    adc    \T4, \T6     
    mulx   \T6, \T7, 8\M1
    mov    \S, \T7             // store T7
    adc    \T0, \T6        
    mulx   \T2, \T6, 16\M1   
    mov    8\S, \T6            // store T6 
    adc    \T1, \T2     
    mulx   \T2, \T7, 24\M1   
    adc    \T3, \T2  
    mulx   \T2, \T6, 32\M1     
    adc    \T5, \T2   
    mulx   \T2, rdx, 40\M1     
    adc    \T2, rax 
        
    xor    rax, rax
    add    \T4, \S 
    adc    \T0, 8\S  
    adc    \T1, \T7  
    adc    \T3, \T6  
    adc    \T5, rdx 
    adc    \T2, rax           
    
    mov    rdx, 24\M0 
    mulx   \T6, \T7, \M1 
    add    \T4, \T7 
    mov    24\C, \T4           // C3_final 
    adc    \T0, \T6     
    mulx   \T6, \T7, 8\M1
    mov    \S, \T7             // store T7
    adc    \T1, \T6        
    mulx   \T4, \T6, 16\M1   
    mov    8\S, \T6            // store T6 
    adc    \T3, \T4     
    mulx   \T4, \T7, 24\M1   
    adc    \T5, \T4  
    mulx   \T4, \T6, 32\M1     
    adc    \T2, \T4   
    mulx   \T4, rdx, 40\M1     
    adc    \T4, rax
        
    xor    rax, rax
    add    \T0, \S 
    adc    \T1, 8\S  
    adc    \T3, \T7  
    adc    \T5, \T6  
    adc    \T2, rdx  
    adc    \T4, rax         
    
    mov    rdx, 32\M0 
    mulx   \T6, \T7, \M1 
    add    \T0, \T7 
    mov    32\C, \T0           // C4_final 
    adc    \T1, \T6     
    mulx   \T6, \T7, 8\M1
    mov    \S, \T7             // store T7
    adc    \T3, \T6        
    mulx   \T0, \T6, 16\M1   
    mov    8\S, \T6            // store T6 
    adc    \T5, \T0     
    mulx   \T0, \T7, 24\M1   
    adc    \T2, \T0  
    mulx   \T0, \T6, 32\M1     
    adc    \T4, \T0   
    mulx   \T0, rdx, 40\M1     
    adc    \T0, rax 
        
    xor    rax, rax
    add    \T1, \S 
    adc    \T3, 8\S  
    adc    \T5, \T7  
    adc    \T2, \T6  
    adc    \T4, rdx  
    adc    \T0, rax           
    
    mov    rdx, 40\M0 
    mulx   \T6, \T7, \M1 
    add    \T1, \T7 
    mov    40\C, \T1           // C5_final 
    adc    \T3, \T6     
    mulx   \T6, \T7, 8\M1
    mov    \S, \T7             // store T7
    adc    \T5, \T6        
    mulx   \T1, \T6, 16\M1   
    mov    8\S, \T6            // store T6 
    adc    \T2, \T1     
    mulx   \T1, \T7, 24\M1   
    adc    \T4, \T1  
    mulx   \T1, \T6, 32\M1     
    adc    \T0, \T1   
    mulx   \T1, rdx, 40\M1     
    adc    \T1, rax 
        
    add    \T3, \S 
    adc    \T5, 8\S  
    adc    \T2, \T7 
    adc    \T4, \T6 
    adc    \T0, rdx 
    adc    \T1, 0 
    mov    48\C, \T3 
    mov    56\C, \T5 
    mov    64\C, \T2 
    mov    72\C, \T4
    mov    80\C, \T0 
    mov    88\C, \T1 
.endm

#endif

.global fp_mul2
fp_mul2:
  mov rdx, rdi
.global fp_mul3
fp_mul3:
    push   r12
    push   r13 
    push   r14 
    push   r15
    mov    rcx, reg_p3 

    // [rsp] <- AH + AL, rax <- mask
    xor    rax, rax
    mov    r8, [reg_p1]
    mov    r9, [reg_p1+8]
    mov    r10, [reg_p1+16]
    mov    r11, [reg_p1+24] 
    mov    r12, [reg_p1+32] 
    mov    r13, [reg_p1+40] 
    push   rbx 
    push   rbp
    sub    rsp, 152
    add    r8, [reg_p1+48]
    adc    r9, [reg_p1+56]
    adc    r10, [reg_p1+64]
    adc    r11, [reg_p1+72]
    adc    r12, [reg_p1+80]
    adc    r13, [reg_p1+88]
    sbb    rax, 0
    mov    [rsp], r8
    mov    [rsp+8], r9
    mov    [rsp+16], r10
    mov    [rsp+24], r11
    mov    [rsp+32], r12
    mov    [rsp+40], r13

    // [rsp+48] <- BH + BL, rdx <- mask
    xor    rdx, rdx
    mov    r8, [reg_p2]
    mov    r9, [reg_p2+8]
    mov    rbx, [reg_p2+16]
    mov    rbp, [reg_p2+24] 
    mov    r14, [reg_p2+32]     
    mov    r15, [reg_p2+40]     
    add    r8, [reg_p2+48]
    adc    r9, [reg_p2+56]
    adc    rbx, [reg_p2+64]
    adc    rbp, [reg_p2+72]
    adc    r14, [reg_p2+80]
    adc    r15, [reg_p2+88]
    sbb    rdx, 0
    mov    [rsp+48], r8
    mov    [rsp+56], r9
    mov    [rsp+64], rbx
    mov    [rsp+72], rbp
    mov    [rsp+80], r14     
    mov    [rsp+88], r15     
    
    // [rcx] <- masked (BH + BL)
    and    r8, rax
    and    r9, rax
    and    rbx, rax
    and    rbp, rax
    and    r14, rax     
    and    r15, rax     
    mov    [rcx], r8
    mov    [rcx+8], r9
    mov    [rcx+16], rbx    /////
    mov    [rcx+24], rbp    /////

    // r8-r13 <- masked (AH + AL)
    mov    r8, [rsp]
    mov    r9, [rsp+8]
    and    r8, rdx
    and    r9, rdx
    and    r10, rdx
    and    r11, rdx
    and    r12, rdx
    and    r13, rdx

    // [rsp+96] <- masked (AH + AL) + masked (AH + AL)
    mov    rax, [rcx]
    mov    rdx, [rcx+8]
    add    r8, rax
    adc    r9, rdx
    adc    r10, rbx
    adc    r11, rbp
    adc    r12, r14         
    adc    r13, r15         
    mov    [rsp+96], r8
    mov    [rsp+104], r9
    mov    [rsp+112], r10
    mov    [rsp+120], r11

    // [rcx] <- AL x BL
    MUL384_SCHOOL  [reg_p1], [reg_p2], [rcx], [rsp+128], r8, r9, r10, r11, rbx, rbp, r14, r15     // Result C0-C5 

    // [rcx+96] <- (AH+AL) x (BH+BL), low part 
    MUL384_SCHOOL  [rsp], [rsp+48], [rcx+96], [rsp+128], r8, r9, r10, r11, rbx, rbp, r14, r15

    // [rsp] <- AH x BH 
    MUL384_SCHOOL  [reg_p1+48], [reg_p2+48], [rsp], [rsp+128], r8, r9, r10, r11, rbx, rbp, r14, r15
    
    // r8-r13 <- (AH+AL) x (BH+BL), final step
    mov    r8, [rsp+96]
    mov    r9, [rsp+104]
    mov    r10, [rsp+112]
    mov    r11, [rsp+120]
    mov    rax, [rcx+144]
    add    r8, rax
    mov    rax, [rcx+152]
    adc    r9, rax
    mov    rax, [rcx+160]
    adc    r10, rax
    mov    rax, [rcx+168]
    adc    r11, rax
    mov    rax, [rcx+176]
    adc    r12, rax
    mov    rax, [rcx+184]
    adc    r13, rax
    
    // rdi,rdx,rbx,rbp,r14,r15,r8-r13 <- (AH+AL) x (BH+BL) - ALxBL
    mov    rdi, [rcx+96]
    sub    rdi, [rcx]
    mov    rdx, [rcx+104]
    sbb    rdx, [rcx+8]
    mov    rbx, [rcx+112]
    sbb    rbx, [rcx+16]
    mov    rbp, [rcx+120]
    sbb    rbp, [rcx+24]
    mov    r14, [rcx+128]     
    sbb    r14, [rcx+32]   
    mov    r15, [rcx+136]     
    sbb    r15, [rcx+40]     
    sbb    r8, [rcx+48]
    sbb    r9, [rcx+56]
    sbb    r10, [rcx+64]
    sbb    r11, [rcx+72]
    sbb    r12, [rcx+80]
    sbb    r13, [rcx+88]
    
    // rdi,rdx,rbx,rbp,r14,r15,r8-r13 <- (AH+AL) x (BH+BL) - ALxBL - AHxBH
    sub    rdi, [rsp]
    sbb    rdx, [rsp+8]
    sbb    rbx, [rsp+16]
    sbb    rbp, [rsp+24]
    sbb    r14, [rsp+32]     
    sbb    r15, [rsp+40]   
    sbb    r8, [rsp+48]
    sbb    r9, [rsp+56]
    sbb    r10, [rsp+64]
    sbb    r11, [rsp+72]
    sbb    r12, [rsp+80]
    sbb    r13, [rsp+88]
    
    mov    rax, [rcx+48]
    add    rax, rdi
    mov    [rcx+48], rax    // Result C6-C11
    mov    rax, [rcx+56]
    adc    rax, rdx
    mov    [rcx+56], rax 
    mov    rax, [rcx+64]
    adc    rax, rbx
    mov    [rcx+64], rax 
    mov    rax, [rcx+72]
    adc    rax, rbp
    mov    [rcx+72], rax 
    mov    rax, [rcx+80]
    adc    rax, r14           
    mov    [rcx+80], rax 
    mov    rax, [rcx+88]
    adc    rax, r15             
    mov    [rcx+88], rax
    mov    rax, [rsp]
    adc    r8, rax 
    mov    [rcx+96], r8    // Result C8-C15
    mov    rax, [rsp+8]
    adc    r9, rax
    mov    [rcx+104], r9 
    mov    rax, [rsp+16]
    adc    r10, rax
    mov    [rcx+112], r10 
    mov    rax, [rsp+24]
    adc    r11, rax
    mov    [rcx+120], r11 
    mov    rax, [rsp+32]
    adc    r12, rax
    mov    [rcx+128], r12 
    mov    rax, [rsp+40]
    adc    r13, rax
    mov    [rcx+136], r13
    mov    r8, [rsp+48]
    mov    r9, [rsp+56]
    mov    r10, [rsp+64]
    mov    r11, [rsp+72]
    mov    r12, [rsp+80]
    mov    r13, [rsp+88]
    adc    r8, 0
    adc    r9, 0
    adc    r10, 0
    adc    r11, 0
    adc    r12, 0
    adc    r13, 0
    add    rsp, 152   
    mov    [rcx+144], r8 
    mov    [rcx+152], r9 
    mov    [rcx+160], r10 
    mov    [rcx+168], r11 
    mov    [rcx+176], r12 
    mov    [rcx+184], r13 
     
    pop    rbp  
    pop    rbx
    pop    r15
    pop    r14
    pop    r13
    pop    r12
    jmp .reduce_once

.global fp_sq1
fp_sq1:
    mov rsi, rdi
.global fp_sq2
fp_sq2:
    /* TODO implement optimized Montgomery squaring */
    mov rdx, rsi
    addq [fp_sq_count+rip],1
    jmp fp_mul3
